{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNetOnPreviousDataSection.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1l1TyU5KPWa_IOoXRUFRxuN-u4Jzxv3mt","authorship_tag":"ABX9TyPMqPNkMpKBwVxUY3HK0J0E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"Nba4s1I7cNI4","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n"," \n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","import os\n","import sys\n","import sklearn\n","import builtins \n","import operator\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from scipy import sparse\n","from scipy.sparse import linalg as slinalg\n","from sklearn.linear_model import Ridge\n","from sklearn.model_selection import train_test_split\n","import gc\n","from tensorflow.keras import regularizers\n","from scipy.interpolate import interp1d\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import time\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skI46K_BcaCH","colab_type":"code","colab":{}},"source":[" \n","DATASET_NAMES_2018 = [   'ACSF1', 'Adiac', \n","                                 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken', 'BME', 'Car', 'CBF', 'Chinatown',\n","                                 'ChlorineConcentration', 'CinCECGTorso', 'Coffee', 'Computers', 'CricketX',\n","                                 'CricketY', 'CricketZ', 'Crop', 'DiatomSizeReduction',\n","                                 'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW',\n","                                 'Earthquakes', 'ECG200',\n","                                 'ECG5000', 'ECGFiveDays', 'ElectricDevices', 'EOGHorizontalSignal',\n","                                 'EOGVerticalSignal', 'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR',\n","                                 'FiftyWords', 'Fish', 'FordA', 'FordB', 'FreezerRegularTrain',\n","                                 'FreezerSmallTrain', 'Fungi', 'GunPoint',\n","                                 'GunPointAgeSpan', 'GunPointMaleVersusFemale', 'GunPointOldVersusYoung',\n","                                 'Ham', 'HandOutlines', 'Haptics', 'Herring', 'HouseTwenty', 'InlineSkate',\n","                                 'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound',\n","                                 'ItalyPowerDemand', 'LargeKitchenAppliances', 'Lightning2', 'Lightning7',\n","                                 'Mallat', 'Meat', 'MedicalImages', \n","                                 'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxOutlineCorrect',\n","                                 'MiddlePhalanxTW', 'MixedShapesRegularTrain', 'MixedShapesSmallTrain',\n","                                 'MoteStrain', 'NonInvasiveFetalECGThorax1', 'NonInvasiveFetalECGThorax2',\n","                                 'OliveOil', 'OSULeaf', 'PhalangesOutlinesCorrect', 'Phoneme',\n","                                 'PickupGestureWiimoteZ', 'PigAirwayPressure', 'PigArtPressure', 'PigCVP',\n","                                  'Plane', 'PowerCons', 'ProximalPhalanxOutlineAgeGroup',\n","                                 'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW', 'RefrigerationDevices',\n","                                 'Rock', 'ScreenType', 'SemgHandGenderCh2', 'SemgHandMovementCh2',\n","                                 'SemgHandSubjectCh2', 'ShapeletSim', 'ShapesAll',\n","                                 'SmallKitchenAppliances', 'SmoothSubspace', 'SonyAIBORobotSurface1',\n","                                 'SonyAIBORobotSurface2', 'StarLightCurves', 'Strawberry', 'SwedishLeaf',\n","                                 'Symbols', 'SyntheticControl', 'ToeSegmentation1', 'ToeSegmentation2', 'Trace',\n","                                 'TwoLeadECG', 'TwoPatterns', 'UMD', 'UWaveGestureLibraryAll',\n","                                 'UWaveGestureLibraryX', 'UWaveGestureLibraryY', 'UWaveGestureLibraryZ',\n","                                 'Wafer', 'Wine', 'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga'\n","                               ]\n"," \n","ITERATIONS = 1  \n"," \n","ARCHIVE_NAMES = ['UCRAll']\n"," \n","dataset_names_for_archive = {'UCRAll': DATASET_NAMES_2018}\n"," \n","CLASSIFIERS = ['resnet']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABe4T_HQcvsz","colab_type":"code","colab":{}},"source":["def readucr(filename):\n","    data = np.loadtxt(filename, delimiter=',')\n","    Y = data[:, 0]\n","    X = data[:, 1:]\n","    return X, Y\n","\n","\n","def create_directory(directory_path):\n","    if os.path.exists(directory_path):\n","        return None\n","    else:\n","        try:\n","            os.makedirs(directory_path)\n","        except:\n","      \n","            return None\n","        return directory_path\n","\n","\n","def create_path(root_dir, classifier_name, archive_name):\n","    output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n","    if os.path.exists(output_directory):\n","        return None\n","    else:\n","        os.makedirs(output_directory)\n","        return output_directory\n","\n","\n","def read_dataset(root_dir, archive_name, dataset_name):\n","    datasets_dict = {}\n","    cur_root_dir = root_dir.replace('-temp', '')\n","\n","    if archive_name == 'MTS':\n","        file_name = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n","        x_train = np.load(file_name + 'x_train.npy')\n","        y_train = np.load(file_name + 'y_train.npy')\n","        x_test = np.load(file_name + 'x_test.npy')\n","        y_test = np.load(file_name + 'y_test.npy')\n","\n","        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                       y_test.copy())\n","\n","    elif archive_name == 'UCR':\n","        root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name\n","        df_train = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TRAIN.csv', sep=',', skiprows = 1, header=None)\n","\n","        df_test = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TEST.csv', skiprows = 1, sep=',', header=None)\n","\n","        y_train = df_train.values[:, df_train.shape[1]-1]\n","        y_test = df_test.values[:, df_test.shape[1]-1]\n","\n","        x_train = df_train.drop(columns=[df_train.shape[1]-1])\n","        x_test = df_test.drop(columns=[df_test.shape[1]-1])\n","\n","        x_train.columns = range(x_train.shape[1])\n","        x_test.columns = range(x_test.shape[1])\n","\n","        x_train = x_train.values\n","        x_test = x_test.values\n","\n","        # znorm\n","        std_ = x_train.std(axis=1, keepdims=True)\n","        std_[std_ == 0] = 1.0\n","        x_train = (x_train - x_train.mean(axis=1, keepdims=True)) / std_\n","\n","        std_ = x_test.std(axis=1, keepdims=True)\n","        std_[std_ == 0] = 1.0\n","        x_test = (x_test - x_test.mean(axis=1, keepdims=True)) / std_\n","\n","        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                       y_test.copy())\n","    else:\n","        file_name = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/' + dataset_name\n","        x_train, y_train = readucr(file_name + '_TRAIN')\n","        x_test, y_test = readucr(file_name + '_TEST')\n","        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                       y_test.copy())\n","\n","    return datasets_dict\n","\n","\n","def read_all_datasets(root_dir, archive_name, split_val=False):\n","    datasets_dict = {}\n","    cur_root_dir = root_dir.replace('-temp', '')\n","    dataset_names_to_sort = []\n","\n","    if archive_name == 'MTS':\n","\n","        for dataset_name in MTS_DATASET_NAMES:\n","            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n","\n","            x_train = np.load(root_dir_dataset + 'x_train.npy')\n","            y_train = np.load(root_dir_dataset + 'y_train.npy')\n","            x_test = np.load(root_dir_dataset + 'x_test.npy')\n","            y_test = np.load(root_dir_dataset + 'y_test.npy')\n","\n","            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                           y_test.copy())\n","    elif archive_name == 'UCRAll':\n","        for dataset_name in DATASET_NAMES_2018:\n","            print(dataset_name)\n","            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name\n","\n","            df_train = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TRAIN.csv', sep=',', skiprows = 1, header=None)\n","\n","            df_test = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TEST.csv', skiprows = 1, sep=',', header=None)\n","\n","            y_train = df_train.values[:, df_train.shape[1]-1]\n","            y_test = df_test.values[:, df_test.shape[1]-1]\n","\n","            x_train = df_train.drop(columns=[df_train.shape[1]-1])\n","            x_test = df_test.drop(columns=[df_test.shape[1]-1])\n","\n","            x_train.columns = range(x_train.shape[1])\n","            x_test.columns = range(x_test.shape[1])\n","            x_train = x_train.values\n","            x_test = x_test.values\n","\n","            # znorm\n","            std_ = x_train.std(axis=1, keepdims=True)\n","            std_[std_ == 0] = 1.0\n","            x_train = (x_train - x_train.mean(axis=1, keepdims=True)) / std_\n","\n","            std_ = x_test.std(axis=1, keepdims=True)\n","            std_[std_ == 0] = 1.0\n","            x_test = (x_test - x_test.mean(axis=1, keepdims=True)) / std_\n","\n","            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                           y_test.copy())\n","\n","    else:\n","        for dataset_name in DATASET_NAMES:\n","            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n","            file_name = root_dir_dataset + dataset_name\n","            x_train, y_train = readucr(file_name + '_TRAIN')\n","            x_test, y_test = readucr(file_name + '_TEST')\n","\n","            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n","                                           y_test.copy())\n","\n","            dataset_names_to_sort.append((dataset_name, len(x_train)))\n","\n","        dataset_names_to_sort.sort(key=operator.itemgetter(1))\n","\n","        for i in range(len(DATASET_NAMES)):\n","            DATASET_NAMES[i] = dataset_names_to_sort[i][0]\n","\n","    return datasets_dict\n","\n","\n","\n","def calculate_metrics(y_true, y_pred, duration, y_true_val=None, y_pred_val=None):\n","    res = pd.DataFrame(data=np.zeros((1, 4), dtype=np.float), index=[0],\n","                       columns=['precision', 'accuracy', 'recall', 'duration'])\n","    res['precision'] = precision_score(y_true, y_pred, average='macro')\n","    res['accuracy'] = accuracy_score(y_true, y_pred)\n","\n","    if not y_true_val is None:\n","        # this is useful when transfer learning is used with cross validation\n","        res['accuracy_val'] = accuracy_score(y_true_val, y_pred_val)\n","\n","    res['recall'] = recall_score(y_true, y_pred, average='macro')\n","    res['duration'] = duration\n","    return res\n","\n","\n","def save_test_duration(file_name, test_duration):\n","    res = pd.DataFrame(data=np.zeros((1, 1), dtype=np.float), index=[0],\n","                       columns=['test_duration'])\n","    res['test_duration'] = test_duration\n","    res.to_csv(file_name, index=False)\n","\n","\n","def generate_results_csv(output_file_name, root_dir):\n","    res = pd.DataFrame(data=np.zeros((0, 7), dtype=np.float), index=[],\n","                       columns=['classifier_name', 'archive_name', 'dataset_name',\n","                                'precision', 'accuracy', 'recall', 'duration'])\n","    for classifier_name in CLASSIFIERS:\n","        for archive_name in ARCHIVE_NAMES:\n","            datasets_dict = read_all_datasets(root_dir, archive_name)\n","            for it in range(ITERATIONS):\n","                curr_archive_name = archive_name\n","                if it != 0:\n","                    curr_archive_name = curr_archive_name + '_itr_' + str(it)\n","                for dataset_name in datasets_dict.keys():\n","                    output_dir = root_dir + '/results/' + classifier_name + '/' \\\n","                                 + curr_archive_name + '/' + dataset_name + '/' + 'df_metrics.csv'\n","                    if not os.path.exists(output_dir):\n","                        continue\n","                    df_metrics = pd.read_csv(output_dir)\n","                    df_metrics['classifier_name'] = classifier_name\n","                    df_metrics['archive_name'] = archive_name\n","                    df_metrics['dataset_name'] = dataset_name\n","                    res = pd.concat((res, df_metrics), axis=0, sort=False)\n","\n","    res.to_csv(root_dir + output_file_name, index=False)\n","    # aggreagte the accuracy for iterations on same dataset\n","    res = pd.DataFrame({\n","        'accuracy': res.groupby(\n","            ['classifier_name', 'archive_name', 'dataset_name'])['accuracy'].mean()\n","    }).reset_index()\n","\n","    return res\n","\n","\n","def plot_epochs_metric(hist, file_name, metric='loss'):\n","    plt.figure()\n","    plt.plot(hist.history[metric])\n","    plt.plot(hist.history['val_' + metric])\n","    plt.title('model ' + metric)\n","    plt.ylabel(metric, fontsize='large')\n","    plt.xlabel('epoch', fontsize='large')\n","    plt.legend(['train', 'val'], loc='upper left')\n","    plt.savefig(file_name, bbox_inches='tight')\n","    plt.close()\n","\n","def save_logs(output_directory, hist, y_pred, y_true, duration, lr=True, y_true_val=None, y_pred_val=None):\n","    hist_df = pd.DataFrame(hist.history)\n","    hist_df.to_csv(output_directory + 'history.csv', index=False)\n","\n","    df_metrics = calculate_metrics(y_true, y_pred, duration, y_true_val, y_pred_val)\n","    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n","\n","    index_best_model = hist_df['loss'].idxmin()\n","    row_best_model = hist_df.loc[index_best_model]\n","\n","    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=np.float), index=[0],\n","                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n","                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n","\n","    df_best_model['best_model_train_loss'] = row_best_model['loss']\n","    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n","    df_best_model['best_model_train_acc'] = row_best_model['accuracy']\n","    df_best_model['best_model_val_acc'] = row_best_model['val_accuracy']\n","    if lr == True:\n","        df_best_model['best_model_learning_rate'] = row_best_model['lr']\n","    df_best_model['best_model_nb_epoch'] = index_best_model\n","\n","    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n","\n","\n","    plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n","\n","    return df_metrics\n","\n","\n","def viz_cam(root_dir, classifier_name , archive_name , dataset_name):\n","    import tensorflow.keras as keras\n","    import sklearn\n","\n","\n","\n","    save_name = dataset_name\n","    max_length = 2000\n","    datasets_dict = read_dataset(root_dir, archive_name, dataset_name)\n","\n","    x_train = datasets_dict[dataset_name][0]\n","    y_train = datasets_dict[dataset_name][1]\n","    y_test = datasets_dict[dataset_name][3]\n","\n","    # transform to binary labels\n","    enc = sklearn.preprocessing.OneHotEncoder()\n","    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n","    y_train_binary = enc.transform(y_train.reshape(-1, 1)).toarray()\n","\n","    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n","\n","    model = keras.models.load_model(\n","        root_dir + '/results/' + classifier_name + '/' + archive_name + '/' + dataset_name + '/best_model.hdf5')\n","\n","    w_k_c = model.layers[-1].get_weights()[0]  \n","\n","\n","    new_input_layer = model.inputs\n","\n","    new_output_layer = [model.layers[-3].output, model.layers[-1].output]\n","\n","    new_feed_forward = keras.backend.function(new_input_layer, new_output_layer)\n","\n","    classes = np.unique(y_train)\n","\n","    for c in classes:\n","        plt.figure()\n","        count = 0\n","        c_x_train = x_train[np.where(y_train == c)]\n","        for ts in c_x_train:\n","            ts = ts.reshape(1, -1, 1)\n","            [conv_out, predicted] = new_feed_forward([ts])\n","            pred_label = np.argmax(predicted)\n","            orig_label = np.argmax(enc.transform([[c]]))\n","            if pred_label == orig_label:\n","                cas = np.zeros(dtype=np.float, shape=(conv_out.shape[1]))\n","                for k, w in enumerate(w_k_c[:, orig_label]):\n","                    cas += w * conv_out[0, :, k]\n","\n","                minimum = np.min(cas)\n","\n","                cas = cas - minimum\n","\n","                cas = cas / max(cas)\n","                cas = cas * 100\n","\n","                x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n","      \n","                f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n","                y = f(x)\n"," \n","                f = interp1d(range(ts.shape[1]), cas)\n","                cas = f(x).astype(int)\n","                plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=2, vmin=0, vmax=100, linewidths=0.0)\n","                if dataset_name == 'Gun_Point':\n","                    if c == 1:\n","                        plt.yticks([-1.0, 0.0, 1.0, 2.0])\n","                    else:\n","                        plt.yticks([-2, -1.0, 0.0, 1.0, 2.0])\n","                count += 1\n","\n","        cbar = plt.colorbar()\n","        print(root_dir)\n","        plt.savefig(root_dir + '/results/' + classifier_name + '/' + archive_name + '/'+ dataset_name + '/DONE/' + 'class-' + str(int(c)) + '.png',\n","                    bbox_inches='tight', dpi=1080)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Qq2N3a0fqIt","colab_type":"code","colab":{}},"source":["class Classifier_RESNET:\n","\n","    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, load_weights=False):\n","        self.output_directory = output_directory\n","        if build == True:\n","            self.model = self.build_model(input_shape, nb_classes)\n","            if (verbose == True):\n","                self.model.summary()\n","            self.verbose = verbose\n","            if load_weights == True:\n","                self.model.load_weights(self.output_directory\n","                                        .replace('resnet_augment', 'resnet')\n","                                        .replace('TSC_itr_augment_x_10', 'TSC_itr_10')\n","                                        + '/model_init.hdf5')\n","            else:\n","                self.model.save_weights(self.output_directory + 'model_init.hdf5')\n","        return\n","\n","    def build_model(self, input_shape, nb_classes):\n","        n_feature_maps = 64\n","\n","        input_layer = keras.layers.Input(input_shape)\n","\n","        # BLOCK 1\n","\n","        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n","        #conv_x = keras.layers.Dropout(.5)(conv_x)\n","        conv_x = keras.layers.BatchNormalization()(conv_x)\n","        conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n","        #conv_y = keras.layers.Dropout(.3)(conv_y)\n","        conv_y = keras.layers.BatchNormalization()(conv_y)\n","        conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n","        #conv_z = keras.layers.Dropout(.2)(conv_z)\n","        conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","        # expand channels for the sum\n","        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n","        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n","\n","        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n","        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n","\n","        # BLOCK 2\n","\n","        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n","        #conv_x = keras.layers.Dropout(.5)(conv_x)\n","        conv_x = keras.layers.BatchNormalization()(conv_x)\n","        conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n","\n","        #conv_y = keras.layers.Dropout(.3)(conv_y)\n","        conv_y = keras.layers.BatchNormalization()(conv_y)\n","        conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n","        #conv_z = keras.layers.Dropout(.2)(conv_z)\n","        conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","        # expand channels for the sum\n","        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n","        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n","\n","        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n","        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n","\n","        # BLOCK 3\n","\n","        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n","        #conv_x = keras.layers.Dropout(.5)(conv_x)\n","        conv_x = keras.layers.BatchNormalization()(conv_x)\n","        conv_x = keras.layers.Activation('relu')(conv_x)\n","\n","        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n","\n","        #conv_y = keras.layers.Dropout(.3)(conv_y)\n","        conv_y = keras.layers.BatchNormalization()(conv_y)\n","        conv_y = keras.layers.Activation('relu')(conv_y)\n","\n","        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n","        #conv_z = keras.layers.Dropout(.2)(conv_z)\n","        conv_z = keras.layers.BatchNormalization()(conv_z)\n","\n","        # no need to expand channels because they are equal\n","        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n","\n","        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n","        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n","\n","        # FINAL\n","\n","        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n","\n","        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n","\n","        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n","\n","        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n","                      metrics=['accuracy'])\n","\n","        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n","\n","        file_path = self.output_directory + 'best_model.hdf5'\n","\n","        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n","                                                           save_best_only=True)\n","\n","        self.callbacks = [reduce_lr, model_checkpoint]\n","\n","        return model\n","\n","    def fit(self, x_train, y_train, x_val, y_val, y_true):\n","        if not tf.test.is_gpu_available:\n","            print('error')\n","            exit()\n","        \n","        batch_size = 64\n","        nb_epochs = 1500\n","\n","        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n","\n","        start_time = time.time()\n","\n","        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n","                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n","\n","        duration = time.time() - start_time\n","\n","        self.model.save(self.output_directory + 'last_model.hdf5')\n","\n","        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n","                              return_df_metrics=False)\n","\n","        \n","        np.save(self.output_directory + 'y_pred.npy', y_pred)\n","\n","       \n","        y_pred = np.argmax(y_pred, axis=1)\n","\n","        df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration)\n","\n","        keras.backend.clear_session()\n","\n","        return df_metrics\n","\n","    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n","        start_time = time.time()\n","        model_path = self.output_directory + 'best_model.hdf5'\n","        model = keras.models.load_model(model_path)\n","        y_pred = model.predict(x_test)\n","        if return_df_metrics:\n","            y_pred = np.argmax(y_pred, axis=1)\n","            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n","            return df_metrics\n","        else:\n","            test_duration = time.time() - start_time\n","            save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n","            return y_pred\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvO2e9n5mSdd","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9HA192PTmWFJ","colab":{}},"source":["class Classifier_FCN:\n","\n","\tdef __init__(self, output_directory, input_shape, nb_classes, verbose=False,build=True):\n","\t\tself.output_directory = output_directory\n","\t\tif build == True:\n","\t\t\tself.model = self.build_model(input_shape, nb_classes)\n","\t\t\tif(verbose==True):\n","\t\t\t\tself.model.summary()\n","\t\t\tself.verbose = verbose\n","\t\t\tself.model.save_weights(self.output_directory+'model_init.hdf5')\n","\t\treturn\n","\n","\tdef build_model(self, input_shape, nb_classes):\n","\t\tinput_layer = keras.layers.Input(input_shape)\n","\n","\t\tconv1 = keras.layers.Conv1D(filters=128, kernel_size=8, padding='same')(input_layer)\n","\t\tconv1 = keras.layers.BatchNormalization()(conv1)\n","\t\tconv1 = keras.layers.Activation(activation='relu')(conv1)\n","\n","\t\tconv2 = keras.layers.Conv1D(filters=256, kernel_size=5, padding='same')(conv1)\n","\t\tconv2 = keras.layers.BatchNormalization()(conv2)\n","\t\tconv2 = keras.layers.Activation('relu')(conv2)\n","\n","\t\tconv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n","\t\tconv3 = keras.layers.BatchNormalization()(conv3)\n","\t\tconv3 = keras.layers.Activation('relu')(conv3)\n","\n","\t\tgap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n","\n","\t\toutput_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n","\n","\t\tmodel = keras.models.Model(inputs=input_layer, outputs=output_layer)\n","\n","\t\tmodel.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), \n","\t\t\tmetrics=['accuracy'])\n","\n","\t\treduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, \n","\t\t\tmin_lr=0.0001)\n","\n","\t\tfile_path = self.output_directory+'best_model.hdf5'\n","\n","\t\tmodel_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss', \n","\t\t\tsave_best_only=True)\n","\n","\t\tself.callbacks = [reduce_lr,model_checkpoint]\n","\n","\t\treturn model \n","\n","\tdef fit(self, x_train, y_train, x_val, y_val,y_true):\n","\t\tif not tf.test.is_gpu_available:\n","\t\t\tprint('error')\n","\t\t\texit()\n","\t\t# x_val and y_val are only used to monitor the test loss and NOT for training  \n","\t\tbatch_size = 16\n","\t\tnb_epochs = 2000\n","\n","\t\tmini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n","\n","\t\tstart_time = time.time() \n","\n","\t\thist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n","\t\t\tverbose=self.verbose, validation_data=(x_val,y_val), callbacks=self.callbacks)\n","\t\t\n","\t\tduration = time.time() - start_time\n","\n","\t\tself.model.save(self.output_directory+'last_model.hdf5')\n","\n","\t\tmodel = keras.models.load_model(self.output_directory+'best_model.hdf5')\n","\n","\t\ty_pred = model.predict(x_val)\n","\n","\t\t# convert the predicted from binary to integer \n","\t\ty_pred = np.argmax(y_pred , axis=1)\n","\n","\t\tsave_logs(self.output_directory, hist, y_pred, y_true, duration)\n","\n","\t\tkeras.backend.clear_session()\n","\n","\tdef predict(self, x_test, y_true,x_train,y_train,y_test,return_df_metrics = True):\n","\t\tmodel_path = self.output_directory + 'best_model.hdf5'\n","\t\tmodel = keras.models.load_model(model_path)\n","\t\ty_pred = model.predict(x_test)\n","\t\tif return_df_metrics:\n","\t\t\ty_pred = np.argmax(y_pred, axis=1)\n","\t\t\tdf_metrics = calculate_metrics(y_true, y_pred, 0.0)\n","\t\t\treturn df_metrics\n","\t\telse:\n","\t\t\treturn y_pred\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vbwGOSq0ftAw","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qIcXv0FzfyAM","colab":{}},"source":["class Classifier_MLP:\n","\n","\tdef __init__(self, output_directory, input_shape, nb_classes, verbose=False,build=True):\n","\t\tself.output_directory = output_directory\n","\t\tif build == True:\n","\t\t\tself.model = self.build_model(input_shape, nb_classes)\n","\t\t\tif(verbose==True):\n","\t\t\t\tself.model.summary()\n","\t\t\tself.verbose = verbose\n","\t\t\tself.model.save_weights(self.output_directory + 'model_init.hdf5')\n","\t\treturn\n","\n","\tdef build_model(self, input_shape, nb_classes):\n","\t\tinput_layer = keras.layers.Input(input_shape)\n","\n","\t\t# flatten/reshape because when multivariate all should be on the same axis \n","\t\tinput_layer_flattened = keras.layers.Flatten()(input_layer)\n","\t\t\n","\t\tlayer_1 = keras.layers.Dropout(0.1)(input_layer_flattened)\n","\t\tlayer_1 = keras.layers.Dense(500, activation='relu')(layer_1)\n","\n","\t\tlayer_2 = keras.layers.Dropout(0.2)(layer_1)\n","\t\tlayer_2 = keras.layers.Dense(500, activation='relu')(layer_2)\n","\n","\t\tlayer_3 = keras.layers.Dropout(0.2)(layer_2)\n","\t\tlayer_3 = keras.layers.Dense(500, activation='relu')(layer_3)\n","\n","\t\toutput_layer = keras.layers.Dropout(0.3)(layer_3)\n","\t\toutput_layer = keras.layers.Dense(nb_classes, activation='softmax')(output_layer)\n","\n","\t\tmodel = keras.models.Model(inputs=input_layer, outputs=output_layer)\n","\n","\t\tmodel.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adadelta(),\n","\t\t\tmetrics=['accuracy'])\n","\n","\t\treduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=200, min_lr=0.1)\n","\n","\t\tfile_path = self.output_directory+'best_model.hdf5' \n","\n","\t\tmodel_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss', \n","\t\t\tsave_best_only=True)\n","\n","\t\tself.callbacks = [reduce_lr,model_checkpoint]\n","\n","\t\treturn model\n","\n","\tdef fit(self, x_train, y_train, x_val, y_val,y_true):\n","\t\tif not tf.test.is_gpu_available:\n","\t\t\tprint('error')\n","\t\t\texit()\n","\t\t# x_val and y_val are only used to monitor the test loss and NOT for training  \n","\t\tbatch_size = 16\n","\t\tnb_epochs = 5000\n","\n","\t\tmini_batch_size = int(min(x_train.shape[0]/10, batch_size))\n","\n","\t\tstart_time = time.time() \n","\n","\t\thist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n","\t\t\tverbose=self.verbose, validation_data=(x_val,y_val), callbacks=self.callbacks)\n","\t\t\n","\t\tduration = time.time() - start_time\n","\n","\t\tself.model.save(self.output_directory + 'last_model.hdf5')\n","\n","\t\tmodel = keras.models.load_model(self.output_directory+'best_model.hdf5')\n","\n","\t\ty_pred = model.predict(x_val)\n","\n","\t\t# convert the predicted from binary to integer \n","\t\ty_pred = np.argmax(y_pred , axis=1)\n","\n","\t\tsave_logs(self.output_directory, hist, y_pred, y_true, duration)\n","\n","\t\tkeras.backend.clear_session()\n","\n","\tdef predict(self, x_test, y_true,x_train,y_train,y_test,return_df_metrics = True):\n","\t\tmodel_path = self.output_directory + 'best_model.hdf5'\n","\t\tmodel = keras.models.load_model(model_path)\n","\t\ty_pred = model.predict(x_test)\n","\t\tif return_df_metrics:\n","\t\t\ty_pred = np.argmax(y_pred, axis=1)\n","\t\t\tdf_metrics = calculate_metrics(y_true, y_pred, 0.0)\n","\t\t\treturn df_metrics\n","\t\telse:\n","\t\t\treturn y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i5LeGPQClGwr","colab_type":"code","colab":{}},"source":["def fit_classifier():\n","    x_train = datasets_dict[dataset_name][0]\n","    y_train = datasets_dict[dataset_name][1]\n","    x_test = datasets_dict[dataset_name][2]\n","    y_test = datasets_dict[dataset_name][3]\n","\n","    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n","\n","    \n","    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n","    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n","    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n","    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n","\n","    \n","    y_true = np.argmax(y_test, axis=1)\n","\n","    if len(x_train.shape) == 2:  \n","        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n","        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n","\n","    input_shape = x_train.shape[1:]\n","    classifier = create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n","\n","    classifier.fit(x_train, y_train, x_test, y_test, y_true)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59e3HhPilOvl","colab_type":"code","colab":{}},"source":["def create_classifier(classifier_name, input_shape, nb_classes, output_directory, verbose=False):\n","    return Classifier_RESNET(output_directory, input_shape, nb_classes, verbose)\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ccMX5j-3QuU","colab_type":"code","colab":{}},"source":["control = 'run_all'\n","itr = '_itr_1'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmSATW11pdLM","colab_type":"code","colab":{}},"source":["root_dir = '/content/drive/My Drive/Colab Notebooks/TSC'\n"," \n","if control == 'run_all':\n","    for classifier_name in CLASSIFIERS:\n","        print('classifier_name', classifier_name)\n"," \n","        for archive_name in ARCHIVE_NAMES:\n","            print('\\tarchive_name', archive_name)\n"," \n","            datasets_dict = read_all_datasets(root_dir, archive_name)\n"," \n","            for iter in range(ITERATIONS):\n","                print('\\t\\titer', iter)\n"," \n","                trr = ''\n","                if iter != 0:\n","                    trr = '_itr_' + str(iter)\n"," \n","                tmp_output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + trr + '/'\n"," \n","                for dataset_name in dataset_names_for_archive[archive_name]:\n","                    print('\\t\\t\\tdataset_name: ', dataset_name)\n"," \n","                    output_directory = tmp_output_directory + dataset_name + '/'\n"," \n","                    create_directory(output_directory)\n"," \n","                    fit_classifier()\n"," \n","                    print('\\t\\t\\t\\tDONE')\n"," \n","                    create_directory(output_directory + '/DONE')\n"," \n"," \n","elif control == 'viz_cam':\n","    viz_cam(root_dir, classifier_name , archive_name , dataset_name)\n","else:\n"," \n","    if itr == '_itr_0':\n","        itr = ''\n"," \n","    output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + itr + '/' + \\\n","                       dataset_name + '/'\n"," \n","    test_dir_df_metrics = output_directory + 'df_metrics.csv'\n"," \n","    print('Method: ', archive_name, dataset_name, classifier_name, itr)\n"," \n","    if os.path.exists(test_dir_df_metrics):\n","        print('Already done')\n","    else:\n"," \n","        create_directory(output_directory)\n","        datasets_dict = read_dataset(root_dir, archive_name, dataset_name)\n"," \n","        fit_classifier()\n"," \n","        print('DONE')\n"," \n","        # the creation of this directory means\n","        create_directory(output_directory + '/DONE')"],"execution_count":null,"outputs":[]}]}